{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89ffd954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import torch, random, numpy as np\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "print(f\"Global seed set to {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "073bc341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints  output.jpg  Weather-Detection-Using-Images\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d39acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/ayannareda/Weather-Detection-Using-Images.git\n",
    "# url - /kaggle/working/Weather-Detection-Using-Images/Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fce2053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ImageFilenameDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.files = []\n",
    "        for label_name in os.listdir(root):\n",
    "            label_dir = os.path.join(root, label_name)\n",
    "            if os.path.isdir(label_dir) and label_name.isdigit():\n",
    "                for fname in sorted(os.listdir(label_dir)):\n",
    "                    if fname.lower().endswith(('.jpg', '.png')):\n",
    "                        path = os.path.join(label_dir, fname)\n",
    "                        self.files.append((path, int(label_name)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.files[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ecbf884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fnn\n",
    "from torchvision import models\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------------------\n",
    "# 1. Sub-modules: Gseg, Gatt, Ginit (conditional on target label)\n",
    "# --------------------------------------\n",
    "class WeatherCueSegmentationModule(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU()\n",
    "        )\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, num_classes, 4, 2, 1), nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.encoder(x)\n",
    "        m = self.middle(e)\n",
    "        return self.decoder(m)\n",
    "\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 3, 1, 1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 1, 1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class InitialTranslationModule(nn.Module):\n",
    "    def __init__(self, cond_channels, feature_dim=64):\n",
    "        super().__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            nn.Conv2d(cond_channels, feature_dim, 4, 2, 1), nn.ReLU(), nn.InstanceNorm2d(feature_dim),\n",
    "            nn.Conv2d(feature_dim, feature_dim*2, 4, 2, 1), nn.ReLU(), nn.InstanceNorm2d(feature_dim*2),\n",
    "            nn.Conv2d(feature_dim*2, feature_dim*4, 4, 2, 1), nn.ReLU(), nn.InstanceNorm2d(feature_dim*4)\n",
    "        )\n",
    "        res = []\n",
    "        for _ in range(6):\n",
    "            res += [nn.Conv2d(feature_dim*4, feature_dim*4, 3, 1, 1), nn.InstanceNorm2d(feature_dim*4), nn.ReLU(),\n",
    "                    nn.Conv2d(feature_dim*4, feature_dim*4, 3, 1, 1), nn.InstanceNorm2d(feature_dim*4)]\n",
    "        self.res_blocks = nn.Sequential(*res)\n",
    "        self.up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(feature_dim*4, feature_dim*2, 4, 2, 1), nn.ReLU(), nn.InstanceNorm2d(feature_dim*2),\n",
    "            nn.ConvTranspose2d(feature_dim*2, feature_dim, 4, 2, 1), nn.ReLU(), nn.InstanceNorm2d(feature_dim),\n",
    "            nn.ConvTranspose2d(feature_dim, 3, 4, 2, 1), nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d = self.down(x)\n",
    "        r = self.res_blocks(d) + d\n",
    "        return self.up(r)\n",
    "\n",
    "# --------------------------------------\n",
    "# 2. Generator G combining modules, conditional on target label\n",
    "# --------------------------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # label embedding to spatial map\n",
    "        self.label_emb = nn.Sequential(\n",
    "            nn.Linear(self.num_classes, in_channels), nn.ReLU()\n",
    "        )\n",
    "        # modules take concatenated [image, label_map]\n",
    "        cond_channels = in_channels * 2\n",
    "        # self.Gseg = WeatherCueSegmentationModule(in_channels, seg_classes)\n",
    "        self.Gatt = AttentionModule(in_channels)\n",
    "        self.Ginit = InitialTranslationModule(cond_channels)\n",
    "\n",
    "    def forward(self, x, target_label):\n",
    "        # target_label: (B,) long or (B,C) one-hot\n",
    "        if target_label.dim() == 1:\n",
    "            onehot = Fnn.one_hot(target_label, num_classes=self.num_classes).float()\n",
    "        else:\n",
    "            onehot = target_label.float()\n",
    "        emb = self.label_emb(onehot)           # (B, in_channels)\n",
    "        B, C, H, W = x.shape\n",
    "        label_map = emb.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, H, W)\n",
    "        cond_input = torch.cat([x, label_map], dim=1)  # (B, 2C, H, W)\n",
    "\n",
    "        att_map = self.Gatt(x)                # (B,1,H,W)\n",
    "        T = att_map                          # (B,1,H,W)\n",
    "        init = self.Ginit(cond_input)         # (B,3,H,W)\n",
    "        T3 = T.repeat(1,3,1,1)\n",
    "        return T3 * init + (1 - T3) * x, None, att_map\n",
    "\n",
    "# --------------------------------------\n",
    "# 3. Discriminator D with class head\n",
    "# --------------------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=5):\n",
    "        super().__init__()\n",
    "        adv = []\n",
    "        dims = [in_channels,64,128,256,512]\n",
    "        for i in range(len(dims)-1): \n",
    "            adv += [nn.Conv2d(dims[i],dims[i+1],4,2,1), nn.LeakyReLU(0.2)]\n",
    "        adv += [nn.Conv2d(512,1,4,1,1)]\n",
    "        self.adv = nn.Sequential(*adv)\n",
    "        \n",
    "        cls = []\n",
    "        dims2 = [in_channels,64,128,256]\n",
    "        for i in range(len(dims2)-1): \n",
    "            cls += [nn.Conv2d(dims2[i],dims2[i+1],4,2,1), nn.LeakyReLU(0.2)]\n",
    "        cls += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(256,num_classes)]\n",
    "        self.cls = nn.Sequential(*cls)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.adv(x), self.cls(x)\n",
    "\n",
    "# --------------------------------------\n",
    "# 4. Losses remain unchanged\n",
    "# --------------------------------------\n",
    "adv_criterion = nn.BCEWithLogitsLoss()\n",
    "l1_criterion = nn.L1Loss()\n",
    "ce_criterion  = nn.CrossEntropyLoss()\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, layers=[0,5,10,19], weights=None):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg19(pretrained=True).features.eval()\n",
    "        # disable inâ€‘place ReLU to avoid autograd errors\n",
    "        for m in vgg.modules():\n",
    "            if isinstance(m, nn.ReLU):\n",
    "                m.inplace = False\n",
    "        for p in vgg.parameters(): p.requires_grad = False\n",
    "        self.vgg, self.layers = vgg, layers\n",
    "        self.ws = weights or [1.0] * len(layers)\n",
    "        \n",
    "    def forward(self,x,y):\n",
    "        loss=0; xi,yi=x,y\n",
    "        for i, l in enumerate(self.vgg): \n",
    "            xi, yi = l(xi), l(yi)\n",
    "            if i in self.layers:\n",
    "                loss += self.ws[self.layers.index(i)] * Fnn.mse_loss(xi, yi)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85c5918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from itertools import zip_longest\n",
    "def train(\n",
    "    G, F, D_X, D_Y,\n",
    "    loader_X, loader_Y, val_loader_X, val_loader_Y,\n",
    "    optim_G, optim_F, optim_D_X, optim_D_Y,\n",
    "    device, epochs=100,\n",
    "    lambda_cycle=0.8,\n",
    "    initial_lr=2e-4,\n",
    "    decay_start_step=1000,\n",
    "    resume_from=None\n",
    "):\n",
    "    perceptual = PerceptualLoss().to(device)\n",
    "\n",
    "    total_steps = epochs * max(len(loader_X), len(loader_Y))\n",
    "    # linear decay after decay_start_step\n",
    "    lr_lambda = lambda step: 1.0 if step < decay_start_step else max(0, float(total_steps-step)/(total_steps-decay_start_step))\n",
    "    sched_G = LambdaLR(optim_G, lr_lambda); sched_F = LambdaLR(optim_F, lr_lambda)\n",
    "    sched_DX = LambdaLR(optim_D_X, lr_lambda); sched_DY = LambdaLR(optim_D_Y, lr_lambda)\n",
    "\n",
    "    ckpt_dir = 'checkpoints'\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    # --- resume from checkpoint if provided ---\n",
    "    start_epoch = 0\n",
    "    best_val_score = None\n",
    "    if resume_from and os.path.isfile(resume_from):\n",
    "        ckpt = torch.load(resume_from, map_location=device)\n",
    "        start_epoch = ckpt['epoch']\n",
    "        best_val_score = ckpt.get('best_val_score', None)\n",
    "        G.load_state_dict(ckpt['G_state']); F.load_state_dict(ckpt['F_state'])\n",
    "        D_X.load_state_dict(ckpt['D_X_state']); D_Y.load_state_dict(ckpt['D_Y_state'])\n",
    "        optim_G.load_state_dict(ckpt['optim_G']); optim_F.load_state_dict(ckpt['optim_F'])\n",
    "        optim_D_X.load_state_dict(ckpt['optim_D_X']); optim_D_Y.load_state_dict(ckpt['optim_D_Y'])\n",
    "        print(f\"Resumed training from epoch {start_epoch}, best_val_score={best_val_score}\")\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        # --- training ---\n",
    "        train_iter = zip_longest(loader_X, loader_Y, fillvalue=(None,None))\n",
    "        train_bar = tqdm(\n",
    "            train_iter,\n",
    "            desc=f\"Epoch {epoch+1}/{epochs}\",\n",
    "            total=max(len(loader_X), len(loader_Y))\n",
    "        )\n",
    "        for step, (batch_X, batch_Y) in enumerate(train_bar, 1):\n",
    "            if batch_X is None or batch_Y is None:\n",
    "                continue\n",
    "            x, x_lbl = batch_X\n",
    "            y, y_lbl = batch_Y\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            x_lbl, y_lbl = x_lbl.to(device), y_lbl.to(device)\n",
    "\n",
    "            # -- Discriminator X update --\n",
    "            optim_D_X.zero_grad()\n",
    "            real_adv_X, real_cls_X = D_X(x)\n",
    "            fake_x, _, _ = F(y, target_label=x_lbl)\n",
    "            fake_adv_X, fake_cls_X = D_X(fake_x.detach())\n",
    "            loss_DX = (adv_criterion(real_adv_X, torch.ones_like(real_adv_X))\n",
    "            + adv_criterion(fake_adv_X, torch.zeros_like(fake_adv_X))\n",
    "            + ce_criterion(real_cls_X, x_lbl)\n",
    "            + ce_criterion(fake_cls_X, x_lbl))\n",
    "            loss_DX.backward()\n",
    "            optim_D_X.step()\n",
    "\n",
    "            # -- Discriminator Y update --\n",
    "            optim_D_Y.zero_grad()\n",
    "            real_adv_Y, real_cls_Y = D_Y(y)\n",
    "            fake_y, _, _ = G(x, target_label=y_lbl)\n",
    "            fake_adv_Y, fake_cls_Y = D_Y(fake_y.detach())\n",
    "            loss_DY = (adv_criterion(real_adv_Y, torch.ones_like(real_adv_Y))\n",
    "            + adv_criterion(fake_adv_Y, torch.zeros_like(fake_adv_Y))\n",
    "            + ce_criterion(real_cls_Y, y_lbl)\n",
    "            + ce_criterion(fake_cls_Y, y_lbl))\n",
    "            loss_DY.backward()\n",
    "            optim_D_Y.step()\n",
    "\n",
    "            # sync discriminator schedulers\n",
    "            sched_DX.step(); sched_DY.step()\n",
    "\n",
    "            # -- Generators update --\n",
    "            optim_G.zero_grad(); optim_F.zero_grad()\n",
    "            fake_y, _, _ = G(x, target_label=y_lbl)\n",
    "            fake_x, _, _ = F(y, target_label=x_lbl)\n",
    "            adv_Y, cls_Y = D_Y(fake_y)\n",
    "            adv_X, cls_X = D_X(fake_x)\n",
    "            loss_G_adv = adv_criterion(adv_Y, torch.ones_like(adv_Y)) + adv_criterion(adv_X, torch.ones_like(adv_X))\n",
    "            loss_cls = ce_criterion(cls_Y, y_lbl) + ce_criterion(cls_X, x_lbl)\n",
    "            rec_x, _, _ = F(fake_y, target_label=x_lbl)\n",
    "            rec_y, _, _ = G(fake_x, target_label=y_lbl)\n",
    "            loss_cycle = (lambda_cycle * (l1_criterion(rec_x, x) + l1_criterion(rec_y, y))\n",
    "                          + (1-lambda_cycle) * (perceptual(x, rec_x) + perceptual(y, rec_y)))\n",
    "            loss_G = loss_G_adv + loss_cls + loss_cycle\n",
    "            loss_G.backward()\n",
    "            optim_G.step(); optim_F.step()\n",
    "\n",
    "            # sync generator schedulers\n",
    "            sched_G.step(); sched_F.step()\n",
    "\n",
    "            # update progress bar metrics\n",
    "            if step % 100 == 0:\n",
    "                train_bar.set_postfix({\n",
    "                    'D_X': loss_DX.item(), 'D_Y': loss_DY.item(),\n",
    "                    'G_adv': loss_G_adv.item(), 'cycle': loss_cycle.item()\n",
    "                })\n",
    "\n",
    "        # --- validation ---\n",
    "        G.eval(); F.eval(); D_X.eval(); D_Y.eval()\n",
    "        val_iter = zip_longest(val_loader_X, val_loader_Y, fillvalue=(None,None))\n",
    "        val_bar = tqdm(\n",
    "            val_iter, desc=\"Validation\",\n",
    "            total=max(len(val_loader_X), len(val_loader_Y))\n",
    "        )\n",
    "        val_metrics = {'G_adv':0,'cycle':0,'cls':0}; count=0\n",
    "        with torch.no_grad():\n",
    "            for batch_Xv, batch_Yv in val_bar:\n",
    "                if batch_Xv is None or batch_Yv is None: continue\n",
    "                x_val, x_lbl_val = batch_Xv; y_val, y_lbl_val = batch_Yv\n",
    "                x_val,y_val = x_val.to(device), y_val.to(device)\n",
    "                x_lbl_val,y_lbl_val = x_lbl_val.to(device), y_lbl_val.to(device)\n",
    "                fake_yv, _, _ = G(x_val, target_label=y_lbl_val)\n",
    "                fake_xv, _, _ = F(y_val, target_label=x_lbl_val)\n",
    "                adv_Yv, cls_Yv = D_Y(fake_yv)\n",
    "                adv_Xv, cls_Xv = D_X(fake_xv)\n",
    "                loss_Gadv_v = (adv_criterion(adv_Yv, torch.ones_like(adv_Yv))\n",
    "                + adv_criterion(adv_Xv, torch.ones_like(adv_Xv)))\n",
    "                rec_xv, _, _ = F(fake_yv, target_label=x_lbl_val)\n",
    "                rec_yv, _, _ = G(fake_xv, target_label=y_lbl_val)\n",
    "                loss_cycle_v = l1_criterion(rec_xv, x_val) + l1_criterion(rec_yv, y_val)\n",
    "                loss_cls_v = ce_criterion(cls_Yv, y_lbl_val) + ce_criterion(cls_Xv, x_lbl_val)\n",
    "                val_metrics['G_adv'] += loss_Gadv_v.item()\n",
    "                val_metrics['cycle'] += loss_cycle_v.item()\n",
    "                val_metrics['cls']   += loss_cls_v.item()\n",
    "                count += 1\n",
    "        # average & report\n",
    "        val_metrics = {k:v/count for k,v in val_metrics.items()}\n",
    "        print(f\"Val G_adv: {val_metrics['G_adv']:.3f}, cycle: {val_metrics['cycle']:.3f}, cls: {val_metrics['cls']:.3f}\")\n",
    "\n",
    "        # save best checkpoint\n",
    "        score = val_metrics['G_adv'] + val_metrics['cycle']\n",
    "        if best_val_score is None or score < best_val_score:\n",
    "            best_val_score = score\n",
    "            ckpt_path = os.path.join(ckpt_dir, f'ckpt_epoch_{epoch+1}.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'G_state': G.state_dict(),\n",
    "                'F_state': F.state_dict(),\n",
    "                'D_X_state': D_X.state_dict(),\n",
    "                'D_Y_state': D_Y.state_dict(),\n",
    "                'optim_G': optim_G.state_dict(),\n",
    "                'optim_F': optim_F.state_dict(),\n",
    "                'optim_D_X': optim_D_X.state_dict(),\n",
    "                'optim_D_Y': optim_D_Y.state_dict(),\n",
    "                'best_val_score': best_val_score\n",
    "            }, ckpt_path)\n",
    "            print(f\"Checkpoint saved: {ckpt_path}\")\n",
    "        G.train(); F.train(); D_X.train(); D_Y.train()\n",
    "\n",
    "    print(\"Training Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17e86ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/20:   0%|          | 0/116 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 224.12 MiB is free. Process 20908 has 14.52 GiB memory in use. Of the allocated memory 13.86 GiB is allocated by PyTorch, and 465.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8107/2759800851.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Call train function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m train(\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Use train_loader for both domains as an example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8107/3122017055.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(G, F, D_X, D_Y, loader_X, loader_Y, val_loader_X, val_loader_Y, optim_G, optim_F, optim_D_X, optim_D_Y, device, epochs, lambda_cycle, initial_lr, decay_start_step, resume_from)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mrec_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_lbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             loss_cycle = (lambda_cycle * (l1_criterion(rec_x, x) + l1_criterion(rec_y, y))\n\u001b[0;32m---> 92\u001b[0;31m                           + (1-lambda_cycle) * (perceptual(x, rec_x) + perceptual(y, rec_y)))\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0mloss_G\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_G_adv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_cls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mloss_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8107/1543675141.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mFnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 224.12 MiB is free. Process 20908 has 14.52 GiB memory in use. Of the allocated memory 13.86 GiB is allocated by PyTorch, and 465.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "# Example usage of train function\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instantiate models\n",
    "G = Generator().to(device)\n",
    "F = Generator().to(device)\n",
    "D_X = Discriminator().to(device)\n",
    "D_Y = Discriminator().to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    G   = nn.DataParallel(G)\n",
    "    F   = nn.DataParallel(F)\n",
    "    D_X = nn.DataParallel(D_X)\n",
    "    D_Y = nn.DataParallel(D_Y)\n",
    "\n",
    "# Optimizers\n",
    "optim_G = torch.optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "optim_F = torch.optim.Adam(F.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "optim_D_X = torch.optim.Adam(D_X.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "optim_D_Y = torch.optim.Adam(D_Y.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "# Create dataset and split into train and validation sets\n",
    "dataset = ImageFilenameDataset('/kaggle/working/Weather-Detection-Using-Images/Data', transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Call train function\n",
    "train(\n",
    "    G, F, D_X, D_Y,\n",
    "    train_loader, train_loader,  # Use train_loader for both domains as an example\n",
    "    val_loader, val_loader,      # Use val_loader for both domains as an example\n",
    "    optim_G, optim_F, optim_D_X, optim_D_Y,\n",
    "    device,\n",
    "    epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ca86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf2a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference cell\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.nn import DataParallel\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 1) define transforms (must match training)\n",
    "transform_in = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "# 2) helper to undo Normalize\n",
    "unnormalize = transforms.Normalize(\n",
    "    mean=[-1.0, -1.0, -1.0],\n",
    "    std =[2.0,  2.0,  2.0]\n",
    ")\n",
    "\n",
    "# 3) load checkpoint & build model\n",
    "ckpt = torch.load('ckpt_epoch_3.pt', map_location=device)\n",
    "# If model was trained with DataParallel, wrap before loading state dict\n",
    "\n",
    "G = Generator().to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    G = nn.DataParallel(G)\n",
    "G.load_state_dict(ckpt['G_state'])\n",
    "G.eval()\n",
    "\n",
    "# 4) inference function\n",
    "def infer(img_path, target_label):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    x   = transform_in(img).unsqueeze(0).to(device)\n",
    "    lbl = torch.tensor([target_label], device=device)\n",
    "    with torch.no_grad():\n",
    "        fake, _, _ = G(x, target_label=lbl)\n",
    "    # undo normalization & clamp\n",
    "    fake = unnormalize(fake.squeeze(0).cpu()).clamp(0,1)\n",
    "    return img, transforms.ToPILImage()(fake)\n",
    "\n",
    "# 5) run on an example\n",
    "base_path = '/kaggle/working/Weather-Detection-Using-Images/Data'\n",
    "img_path = os.path.join(base_path, '1', '2256833238.jpg')\n",
    "\n",
    "original, generated = infer(img_path, target_label=2)    # choose your label index\n",
    "\n",
    "# Display side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].imshow(original); axes[0].axis('off'); axes[0].set_title('Original Image')\n",
    "axes[1].imshow(generated); axes[1].axis('off'); axes[1].set_title('Generated Image')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
