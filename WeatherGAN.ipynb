{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ffd954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, numpy as np\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "print(f\"Global seed set to {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b1e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login 8285c1e86ba66976957cd9bdbae9e646b37bba8f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d39acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/ayannareda/Weather-Detection-Using-Images.git\n",
    "# url - /kaggle/working/Weather-Detection-Using-Images/Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9d82e",
   "metadata": {},
   "source": [
    "### Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fce2053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ImageFilenameDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.files = []\n",
    "        for label_name in os.listdir(root):\n",
    "            label_dir = os.path.join(root, label_name)\n",
    "            if os.path.isdir(label_dir) and label_name.isdigit():\n",
    "                for fname in sorted(os.listdir(label_dir)):\n",
    "                    if fname.lower().endswith(('.jpg', '.png')):\n",
    "                        path = os.path.join(label_dir, fname)\n",
    "                        self.files.append((path, int(label_name)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.files[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1582a4b6",
   "metadata": {},
   "source": [
    "### Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecbf884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StarGAN Generator with ResNet blocks and Self-Attention\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim, 3, 1, 1), nn.InstanceNorm2d(dim), nn.ReLU(),\n",
    "            nn.Conv2d(dim, dim, 3, 1, 1), nn.InstanceNorm2d(dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.query = nn.Conv2d(in_dim, in_dim//8, 1)\n",
    "        self.key   = nn.Conv2d(in_dim, in_dim//8, 1)\n",
    "        self.value = nn.Conv2d(in_dim, in_dim,    1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        # project\n",
    "        proj_q = self.query(x).view(b, -1, h*w).permute(0,2,1)   # (B, H*W, C//8)\n",
    "        proj_k = self.key(x).view(b, -1, h*w)                    # (B, C//8, H*W)\n",
    "        attn   = torch.softmax(torch.bmm(proj_q, proj_k), dim=-1)  # (B, H*W, H*W)\n",
    "        proj_v = self.value(x).view(b, -1, h*w)                  # (B, C, H*W)\n",
    "\n",
    "        # original self‐attended output\n",
    "        out = torch.bmm(proj_v, attn.permute(0,2,1)).view(b, c, h, w)\n",
    "        out = self.gamma * out + x\n",
    "\n",
    "        # compute a single‐channel spatial attention map\n",
    "        # sum over *source* locations for each *target* pixel\n",
    "        attn_map = attn.sum(dim=1).view(b, 1, h, w)  # (B,1,H,W)\n",
    "        # normalize to [0,1]\n",
    "        attn_map = attn_map / (attn_map.max(dim=-1, keepdim=True)[0].max(dim=-2, keepdim=True)[0] + 1e-8)\n",
    "\n",
    "        # element‐wise multiply so that high‐attn regions are amplified\n",
    "        out = out * (1 + attn_map)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_ch=3, n_classes=5, n_blocks=6):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        # initial conv\n",
    "        model = [nn.Conv2d(in_ch + n_classes, 64, 7, 1, 3), nn.InstanceNorm2d(64), nn.ReLU()]\n",
    "        # downsample\n",
    "        dim = 64\n",
    "        for _ in range(2):\n",
    "            model += [nn.Conv2d(dim, dim*2, 4, 2, 1), nn.InstanceNorm2d(dim*2), nn.ReLU()]\n",
    "            dim *= 2\n",
    "        # ResNet blocks\n",
    "        for _ in range(n_blocks):\n",
    "            model += [ResnetBlock(dim)]\n",
    "        # insert self-attention layer to focus on weather regions\n",
    "        model += [SelfAttention(dim)]\n",
    "        # upsample\n",
    "        for _ in range(2):\n",
    "            model += [nn.ConvTranspose2d(dim, dim//2, 4, 2, 1), nn.InstanceNorm2d(dim//2), nn.ReLU()]\n",
    "            dim //= 2\n",
    "        # final conv\n",
    "        model += [nn.Conv2d(dim, in_ch, 7, 1, 3), nn.Tanh()]\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x, target_label):\n",
    "        # target_label: one-hot tensor of shape (B, n_classes)\n",
    "        b, _, h, w = x.size()\n",
    "        label_map = target_label.view(b, self.n_classes, 1, 1).expand(b, self.n_classes, h, w)\n",
    "        inp = torch.cat([x, label_map], dim=1) # [B, in_ch + n_classes, h, w]\n",
    "        return self.model(inp) # [B, in_ch, h, w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e205b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StarGAN Discriminator with adversarial and classification heads\n",
    "import torch.nn as nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_ch=3, n_classes=5):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        dim = in_ch\n",
    "        for out_dim in [64, 128, 256, 512]:\n",
    "            layers += [nn.Conv2d(dim, out_dim, 4, 2, 1), nn.LeakyReLU(0.01)]\n",
    "            dim = out_dim\n",
    "        self.main = nn.Sequential(*layers)\n",
    "        self.adv_head = nn.Conv2d(512, 1, 3, 1, 1)\n",
    "        self.cls_head = nn.Conv2d(512, n_classes, 3, 1, 1)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h       = self.main(x)           # [B,512,H',W']\n",
    "        out_adv = self.adv_head(h)       # [B,1, H',W']\n",
    "        out_cls = self.cls_head(h)       # [B,n_classes,H',W']\n",
    "        out_cls = self.global_pool(out_cls)  # [B,n_classes,1,1]\n",
    "        out_cls = out_cls.view(h.size(0), -1)  # [B,n_classes]\n",
    "        return out_adv, out_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc0fe2a",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c5918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from pytorch_msssim import ssim\n",
    "import wandb\n",
    "\n",
    "# Perceptual model (VGG19 features)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vgg = models.vgg19(pretrained=True).features[:16].to(device).eval()\n",
    "for p in vgg.parameters(): p.requires_grad = False\n",
    "\n",
    "def perceptual_loss_fn(x, y):\n",
    "    fx = vgg(x)\n",
    "    fy = vgg(y)\n",
    "    return torch.nn.functional.l1_loss(fx, fy)\n",
    "\n",
    "def train(G, D, train_loader, val_loader, optim_G, optim_D, device,\n",
    "          epochs=50, lambda_cls=1.0, lambda_rec=10.0,\n",
    "          lambda_perc=0.1, lambda_ssim=1.0,\n",
    "          checkpoint_dir='checkpoints', resume_path=None):\n",
    "    # initialize metrics storage\n",
    "    metrics = {\n",
    "        'train_adv': [], 'train_cls': [], 'train_rec': [], 'train_perc': [], 'train_ssim': [],\n",
    "        'val_adv':   [], 'val_cls':   [], 'val_rec':   [], 'val_perc':   [], 'val_ssim':   []\n",
    "    }\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    start_epoch = 1\n",
    "    # -- resume if provided --\n",
    "    if resume_path and os.path.exists(resume_path):\n",
    "        ckpt = torch.load(resume_path, map_location=device)\n",
    "        G.load_state_dict(ckpt['G_state']); D.load_state_dict(ckpt['D_state'])\n",
    "        optim_G.load_state_dict(ckpt['optim_G_state'])\n",
    "        optim_D.load_state_dict(ckpt['optim_D_state'])\n",
    "        start_epoch = ckpt.get('epoch', 1) + 1\n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "    adv_criterion = torch.nn.MSELoss()\n",
    "    cls_criterion = torch.nn.CrossEntropyLoss()\n",
    "    rec_criterion = torch.nn.L1Loss()\n",
    "    G.to(device); D.to(device)\n",
    "\n",
    "    for epoch in range(start_epoch, epochs+1):\n",
    "        G.train(); D.train()\n",
    "        train_stats = {'adv': [], 'cls': [], 'rec': [], 'perc': [], 'ssim': []}\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} [Train]\")\n",
    "        for real_x, real_lbl in pbar:\n",
    "            real_x, real_lbl = real_x.to(device), real_lbl.to(device)\n",
    "            B = real_x.size(0)\n",
    "            idx = torch.randperm(B); target_lbl = real_lbl[idx]\n",
    "            target_oh = F.one_hot(target_lbl, G.module.n_classes if isinstance(G, torch.nn.DataParallel) else G.n_classes).float().to(device)\n",
    "            # print(f\"real_lbl: {real_lbl.shape}, target_lbl: {target_lbl.shape}, target_oh: {target_oh.shape}\")\n",
    "            # Discriminator\n",
    "            fake_x = G(real_x, target_oh).detach()\n",
    "            optim_D.zero_grad()\n",
    "            real_adv, real_cls = D(real_x)\n",
    "            fake_adv, _ = D(fake_x)\n",
    "            loss_D = adv_criterion(real_adv, torch.ones_like(real_adv)) + \\\n",
    "                     adv_criterion(fake_adv, torch.zeros_like(fake_adv)) + \\\n",
    "                     lambda_cls * cls_criterion(real_cls, real_lbl)\n",
    "            loss_D.backward(); optim_D.step()\n",
    "            \n",
    "            # Generator\n",
    "            optim_G.zero_grad()\n",
    "            fake_x = G(real_x, target_oh)\n",
    "            adv_out, cls_out = D(fake_x)\n",
    "            rec_lbl_oh = F.one_hot(real_lbl, G.module.n_classes if isinstance(G, torch.nn.DataParallel) else G.n_classes).float().to(device)\n",
    "            rec_x = G(fake_x, rec_lbl_oh)\n",
    "            # compute additional losses\n",
    "            perc_loss = perceptual_loss_fn(fake_x, real_x)\n",
    "            ssim_loss = 1 - ssim(fake_x, real_x, data_range=1.0, size_average=True)\n",
    "            # combined Generator loss\n",
    "            loss_G = adv_criterion(adv_out, torch.ones_like(adv_out)) + \\\n",
    "                     lambda_cls * cls_criterion(cls_out, target_lbl) + \\\n",
    "                     lambda_rec * rec_criterion(rec_x, real_x) + \\\n",
    "                     lambda_perc * perc_loss + \\\n",
    "                     lambda_ssim * ssim_loss\n",
    "            loss_G.backward(); optim_G.step()\n",
    "\n",
    "            # collect stats\n",
    "            train_stats['adv'].append(loss_G.item())\n",
    "            train_stats['cls'].append(cls_criterion(cls_out, target_lbl).item())\n",
    "            train_stats['rec'].append(rec_criterion(rec_x, real_x).item())\n",
    "            train_stats['perc'].append(perc_loss.item())\n",
    "            train_stats['ssim'].append((1 - ssim_loss).item())\n",
    "            pbar.set_postfix({k: np.mean(v) for k, v in train_stats.items()})\n",
    "\n",
    "        # Validation\n",
    "        G.eval(); D.eval()\n",
    "        val_stats = {'adv': [], 'cls': [], 'rec': [], 'perc': [], 'ssim': []}\n",
    "        with torch.no_grad():\n",
    "            vbar = tqdm(val_loader, desc=f\"Epoch {epoch}/{epochs} [Val]\")\n",
    "            for real_x, real_lbl in vbar:\n",
    "                real_x, real_lbl = real_x.to(device), real_lbl.to(device)\n",
    "                B = real_x.size(0)\n",
    "                idx = torch.randperm(B); target_lbl = real_lbl[idx]\n",
    "                target_oh = F.one_hot(target_lbl, G.module.n_classes if isinstance(G, torch.nn.DataParallel) else G.n_classes).float().to(device)\n",
    "                fake_x = G(real_x, target_oh)\n",
    "                adv_out, cls_out = D(fake_x)\n",
    "                rec_lbl_oh = F.one_hot(real_lbl, G.module.n_classes if isinstance(G, torch.nn.DataParallel) else G.n_classes).float().to(device)\n",
    "                rec_x = G(fake_x, rec_lbl_oh)\n",
    "                val_perc = perceptual_loss_fn(fake_x, real_x)\n",
    "                val_ssim = ssim(fake_x, real_x, data_range=1.0, size_average=True)\n",
    "                val_stats['adv'].append(adv_criterion(adv_out, torch.ones_like(adv_out)).item())\n",
    "                val_stats['cls'].append(cls_criterion(cls_out, target_lbl).item())\n",
    "                val_stats['rec'].append(rec_criterion(rec_x, real_x).item())\n",
    "                val_stats['perc'].append(val_perc.item())\n",
    "                val_stats['ssim'].append(val_ssim.item())\n",
    "                vbar.set_postfix({k: np.mean(v) for k, v in val_stats.items()})\n",
    "\n",
    "        # Log metrics\n",
    "        print(f\"Epoch {epoch}: Train -> adv={np.mean(train_stats['adv']):.4f}, \",\n",
    "              f\"cls={np.mean(train_stats['cls']):.4f}, rec={np.mean(train_stats['rec']):.4f}, perc={np.mean(train_stats['perc']):.4f}, ssim={np.mean(train_stats['ssim']):.4f}\")\n",
    "        print(f\"           Val   -> adv={np.mean(val_stats['adv']):.4f}, \",\n",
    "              f\"cls={np.mean(val_stats['cls']):.4f}, rec={np.mean(val_stats['rec']):.4f}, perc={np.mean(val_stats['perc']):.4f}, ssim={np.mean(val_stats['ssim']):.4f}\")\n",
    "\n",
    "        # wandb logging\n",
    "        wandb.log({\n",
    "            'epoch': epoch,\n",
    "            'train/adv': np.mean(train_stats['adv']),\n",
    "            'train/cls': np.mean(train_stats['cls']),\n",
    "            'train/rec': np.mean(train_stats['rec']),\n",
    "            'train/perc': np.mean(train_stats['perc']),\n",
    "            'train/ssim': np.mean(train_stats['ssim']),\n",
    "            'val/adv': np.mean(val_stats['adv']),\n",
    "            'val/cls': np.mean(val_stats['cls']),\n",
    "            'val/rec': np.mean(val_stats['rec']),\n",
    "            'val/perc': np.mean(val_stats['perc']),\n",
    "            'val/ssim': np.mean(val_stats['ssim'])\n",
    "        })\n",
    "\n",
    "        # Save checkpoint\n",
    "        if epoch % 5 == 0 or epoch == epochs:\n",
    "            ckpt_path = os.path.join(checkpoint_dir, f\"sa_ckpt_epoch_{epoch}.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'G_state': G.state_dict(),\n",
    "                'D_state': D.state_dict(),\n",
    "                'optim_G_state': optim_G.state_dict(),\n",
    "                'optim_D_state': optim_D.state_dict()\n",
    "            }, ckpt_path)\n",
    "        # append epoch metrics\n",
    "        metrics['train_adv'].append(np.mean(train_stats['adv']))\n",
    "        metrics['train_cls'].append(np.mean(train_stats['cls']))\n",
    "        metrics['train_rec'].append(np.mean(train_stats['rec']))\n",
    "        metrics['train_perc'].append(np.mean(train_stats['perc']))\n",
    "        metrics['train_ssim'].append(np.mean(train_stats['ssim']))\n",
    "        metrics['val_adv'].append(np.mean(val_stats['adv']))\n",
    "        metrics['val_cls'].append(np.mean(val_stats['cls']))\n",
    "        metrics['val_rec'].append(np.mean(val_stats['rec']))\n",
    "        metrics['val_perc'].append(np.mean(val_stats['perc']))\n",
    "        metrics['val_ssim'].append(np.mean(val_stats['ssim']))\n",
    "    # end epochs\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f45da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb pytorch-msssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e86ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# Example usage of train_starGAN function\n",
    "import wandb\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instantiate models\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    G = nn.DataParallel(G)\n",
    "    D = nn.DataParallel(D)\n",
    "\n",
    "# Optimizers\n",
    "optim_G = torch.optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "optim_D = torch.optim.Adam(D.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "# Create dataset and split into train and validation sets\n",
    "BATCH_SIZE = 32\n",
    "dataset = ImageFilenameDataset('/kaggle/working/Weather-Detection-Using-Images/Data', transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project='WeatherGAN', name='stargan-perceptual-sa', resume= \"allow\",\n",
    "    id='hvrmqj65',\n",
    "    config={\n",
    "    'epochs': 80,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'lr': 2e-4,\n",
    "    'lambda_cls': 1.0,\n",
    "    'lambda_rec': 10.0,\n",
    "    'lambda_perc': 0.1,\n",
    "    'lambda_ssim': 1.0\n",
    "})\n",
    "\n",
    "# Call train_starGAN function\n",
    "metrics = train(\n",
    "    G, D,\n",
    "    train_loader, val_loader,\n",
    "    optim_G, optim_D,\n",
    "    device,\n",
    "    epochs=80,\n",
    "    checkpoint_dir='checkpoints/perceptual',\n",
    "    resume_path=\"checkpoints/perceptual/sa_ckpt_epoch_40.pt\"\n",
    ")\n",
    "\n",
    "# No need to save metrics as .pkl, all metrics are logged to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88093a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./checkpoints/perceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d96a88e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b4285",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1981f358",
   "metadata": {},
   "source": [
    "## Inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf2a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference for 10 images from class '0' and all target labels\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 1) define transforms (must match training)\n",
    "transform_in = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "# 2) helper to undo Normalize\n",
    "unnormalize = transforms.Normalize(\n",
    "    mean=[-1.0, -1.0, -1.0],\n",
    "    std =[2.0,  2.0,  2.0]\n",
    ")\n",
    "\n",
    "# 3) load checkpoint & build model\n",
    "ckpt = torch.load('./checkpoints/perceptual/sa_ckpt_epoch_45.pt', map_location=device)\n",
    "# If model was trained with DataParallel, wrap before loading state dict\n",
    "\n",
    "G = Generator().to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    G = nn.DataParallel(G)\n",
    "G.load_state_dict(ckpt['G_state'])\n",
    "G.eval()\n",
    "\n",
    "# 4) inference function\n",
    "def infer(img_path, target_label):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    x = transform_in(img).unsqueeze(0).to(device)\n",
    "    lbl = torch.tensor([target_label], device=device)\n",
    "    # handle DataParallel vs single‐GPU\n",
    "    num_classes = G.module.n_classes if hasattr(G, 'module') else G.n_classes\n",
    "    lbl_onehot = F.one_hot(lbl, num_classes=num_classes).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        fake = G(x, target_label=lbl_onehot)\n",
    "    # undo normalization & clamp\n",
    "    fake = unnormalize(fake.squeeze(0).cpu()).clamp(0,1)\n",
    "    return img, transforms.ToPILImage()(fake)\n",
    "\n",
    "# Inference for 10 images from class '0' and all target labels\n",
    "base_path = '/kaggle/working/Weather-Detection-Using-Images/Data'\n",
    "dir0 = os.path.join(base_path, '2')\n",
    "files0 = sorted([f for f in os.listdir(dir0) if f.lower().endswith(('.jpg', '.png'))])\n",
    "files0 = random.sample(files0, min(7, len(files0)))\n",
    "num_labels = G.module.n_classes if hasattr(G, 'module') else G.n_classes\n",
    "fig, axes = plt.subplots(len(files0), 1 + num_labels, figsize=(3*(1+num_labels), 3*len(files0)))\n",
    "for i, fname in enumerate(files0):\n",
    "    path = os.path.join(dir0, fname)\n",
    "    img_orig = Image.open(path).convert('RGB')\n",
    "    img_resized = img_orig.resize((256,256))\n",
    "    axes[i, 0].imshow(img_resized); axes[i, 0].axis('off'); axes[i, 0].set_title('Orig')\n",
    "    for lb in range(num_labels):\n",
    "        _, gen = infer(path, target_label=lb)\n",
    "        axes[i, lb+1].imshow(gen); axes[i, lb+1].axis('off'); axes[i, lb+1].set_title(f'Lbl {lb}')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac44d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /kaggle/working/Weather-Detection-Using-Images/Data/2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
