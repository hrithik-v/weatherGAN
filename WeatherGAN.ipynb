{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89ffd954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import torch, random, numpy as np\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "print(f\"Global seed set to {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17fde331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'checkpoints/wo_perceptual_sa/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls checkpoints/wo_perceptual_sa/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b1e671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login 8285c1e86ba66976957cd9bdbae9e646b37bba8f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d39acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/ayannareda/Weather-Detection-Using-Images.git\n",
    "# url - /kaggle/working/Weather-Detection-Using-Images/Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9d82e",
   "metadata": {},
   "source": [
    "### Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fce2053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ImageFilenameDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.files = []\n",
    "        for label_name in os.listdir(root):\n",
    "            label_dir = os.path.join(root, label_name)\n",
    "            if os.path.isdir(label_dir) and label_name.isdigit():\n",
    "                for fname in sorted(os.listdir(label_dir))[:2200]:\n",
    "                    if fname.lower().endswith(('.jpg', '.png')):\n",
    "                        path = os.path.join(label_dir, fname)\n",
    "                        self.files.append((path, int(label_name)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.files[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1582a4b6",
   "metadata": {},
   "source": [
    "### Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ecbf884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StarGAN Generator with ResNet blocks and Self-Attention\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual Block with instance normalization.\"\"\"\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out, affine=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.main(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator network.\"\"\"\n",
    "    def __init__(self, conv_dim=64, n_classes=5, repeat_num=4):\n",
    "        super(Generator, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(3+self.n_classes, conv_dim, kernel_size=7, stride=1, padding=3, bias=False))\n",
    "        layers.append(nn.InstanceNorm2d(conv_dim, affine=True))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        # Down-sampling layers.\n",
    "        curr_dim = conv_dim\n",
    "        for i in range(2):\n",
    "            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1, bias=False))\n",
    "            layers.append(nn.InstanceNorm2d(curr_dim*2, affine=True))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            curr_dim = curr_dim * 2\n",
    "\n",
    "        # Bottleneck layers.\n",
    "        for i in range(repeat_num):\n",
    "            layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))\n",
    "\n",
    "        # Up-sampling layers.\n",
    "        for i in range(2):\n",
    "            layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "            layers.append(nn.Conv2d(curr_dim, curr_dim//2, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "            layers.append(nn.InstanceNorm2d(curr_dim//2, affine=True))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            curr_dim = curr_dim // 2\n",
    "\n",
    "        layers.append(nn.Conv2d(curr_dim, 3, kernel_size=7, stride=1, padding=3, bias=False))\n",
    "        layers.append(nn.Tanh())\n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # Replicate spatially and concatenate domain information.\n",
    "        # Note that this type of label conditioning does not work at all if we use reflection padding in Conv2d.\n",
    "        # This is because instance normalization ignores the shifting (or bias) effect.\n",
    "        c = c.view(c.size(0), c.size(1), 1, 1)\n",
    "        c = c.repeat(1, 1, x.size(2), x.size(3))\n",
    "        x = torch.cat([x, c], dim=1)\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e205b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PatchGAN Discriminator\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import spectral_norm\n",
    "import numpy as np\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator network with PatchGAN.\"\"\"\n",
    "    def __init__(self, image_size=256, conv_dim=64, c_dim=5, repeat_num=4):\n",
    "        super(Discriminator, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(spectral_norm(nn.Conv2d(3, conv_dim, kernel_size=4, stride=2, padding=1)))\n",
    "        layers.append(nn.LeakyReLU(0.01))\n",
    "\n",
    "        curr_dim = conv_dim\n",
    "        for i in range(1, repeat_num):\n",
    "            layers.append(spectral_norm(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1)))\n",
    "            layers.append(nn.LeakyReLU(0.01))\n",
    "            curr_dim = curr_dim * 2\n",
    "\n",
    "        kernel_size = int(image_size / np.power(2, repeat_num))\n",
    "        self.main = nn.Sequential(*layers)\n",
    "        self.conv1 = spectral_norm(nn.Conv2d(curr_dim, 1, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "        self.conv2 = spectral_norm(nn.Conv2d(curr_dim, c_dim, kernel_size=kernel_size, bias=False))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.main(x)\n",
    "        out_src = self.conv1(h)\n",
    "        out_cls = self.conv2(h)\n",
    "        return out_src, out_cls.view(out_cls.size(0), out_cls.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b163324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "\n",
    "device_ids = list(range(torch.cuda.device_count())) if torch.cuda.is_available() else []\n",
    "\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self, device, layers=[3,8,15,22]):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.selected = set(layers)\n",
    "        all_layers = list(vgg16(pretrained=True).features)\n",
    "        # prepare alternating devices\n",
    "        devices = [torch.device('cuda:0'), torch.device(f'cuda:{device_ids[1]}')] if len(device_ids)>1 else [device, device]\n",
    "        self.layers = nn.ModuleList()\n",
    "        for idx, layer in enumerate(all_layers):\n",
    "            dev = devices[idx % len(devices)]\n",
    "            layer.to(dev).eval()\n",
    "            for p in layer.parameters(): p.requires_grad = False\n",
    "            self.layers.append(layer)\n",
    "        self.devices = devices\n",
    "\n",
    "    def forward(self, gen, real):\n",
    "        xg = (gen + 1) / 2\n",
    "        xr = (real + 1) / 2\n",
    "        feats_g, feats_r = [], []\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            dev = self.devices[idx % len(self.devices)]\n",
    "            xg = layer(xg.to(dev))\n",
    "            xr = layer(xr.to(dev))\n",
    "            if idx in self.selected:\n",
    "                feats_g.append(xg.to(self.device))\n",
    "                feats_r.append(xr.to(self.device))\n",
    "        loss = sum(F.l1_loss(g, r) for g, r in zip(feats_g, feats_r))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc0fe2a",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85c5918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import wandb\n",
    "from torchvision.models import vgg16\n",
    "from torch.amp import autocast, GradScaler\n",
    "import os, random\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define inference transforms and unnormalize\n",
    "transform_in = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "unnormalize = transforms.Normalize(\n",
    "    mean=[-1.0, -1.0, -1.0],\n",
    "    std =[2.0,  2.0,  2.0]\n",
    ")\n",
    "\n",
    "def train(G, D, perceptual_loss, train_loader, val_loader, optim_G, optim_D, device,\n",
    "          epochs=50, lambda_rec=10.0, lambda_perc=0.1, lambda_cls=1.0,\n",
    "          checkpoint_dir='checkpoints', resume_path=None, dataset=None):\n",
    "    # initialize metrics storage\n",
    "    metrics = {\n",
    "        'train_adv': [], 'train_rec': [], 'train_perc': [], 'train_cls': [],\n",
    "        'val_adv':   [], 'val_rec':   [], 'val_perc':   [], 'val_cls': []\n",
    "    }\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    start_epoch = 1\n",
    "    # -- resume if provided --\n",
    "    if resume_path and os.path.exists(resume_path):\n",
    "        ckpt = torch.load(resume_path, map_location=device)\n",
    "        G.load_state_dict(ckpt['G_state']); D.load_state_dict(ckpt['D_state'])\n",
    "        optim_G.load_state_dict(ckpt['optim_G_state']); optim_D.load_state_dict(ckpt['optim_D_state'])\n",
    "        start_epoch = ckpt.get('epoch', 1) + 1\n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "    adv_criterion = torch.nn.MSELoss()\n",
    "    rec_criterion = torch.nn.L1Loss()\n",
    "    cls_criterion = torch.nn.CrossEntropyLoss()\n",
    "    G.to(device); D.to(device)\n",
    "    scaler = GradScaler('cuda')\n",
    "\n",
    "    for epoch in range(start_epoch, epochs+1):\n",
    "        G.train(); D.train()\n",
    "        train_stats = {'adv': [], 'rec': [], 'perc': [], 'cls': []}\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} [Train]\")\n",
    "        for real_x, real_lbl in pbar:\n",
    "            real_x = real_x.to(device)\n",
    "            B = real_x.size(0)\n",
    "            idx = torch.randperm(B)\n",
    "            target_lbl = real_lbl[idx].to(device)\n",
    "            target_oh = F.one_hot(target_lbl, G.module.n_classes if hasattr(G,'module') else G.n_classes).float().to(device)\n",
    "\n",
    "            # Discriminator update\n",
    "            optim_D.zero_grad()\n",
    "            with autocast('cuda'):\n",
    "                fake_x = G(real_x, target_oh).detach()\n",
    "                real_src, real_cls = D(real_x)\n",
    "                fake_src, _ = D(fake_x)\n",
    "                loss_D_adv = adv_criterion(real_src, torch.ones_like(real_src)) + \\\n",
    "                             adv_criterion(fake_src, torch.zeros_like(fake_src))\n",
    "                loss_D_cls = cls_criterion(real_cls, real_lbl.to(device))\n",
    "                loss_D = loss_D_adv + lambda_cls * loss_D_cls\n",
    "            scaler.scale(loss_D).backward()\n",
    "            scaler.step(optim_D)\n",
    "            scaler.update()\n",
    "\n",
    "            # Generator update\n",
    "            optim_G.zero_grad()\n",
    "            with autocast('cuda'):\n",
    "                fake_x = G(real_x, target_oh)\n",
    "                adv_out, cls_out = D(fake_x)\n",
    "                rec_lbl_oh = F.one_hot(real_lbl.to(device), G.module.n_classes if hasattr(G,'module') else G.n_classes).float().to(device)\n",
    "                rec_x = G(fake_x, rec_lbl_oh)\n",
    "                perc_loss = perceptual_loss(fake_x, real_x)\n",
    "                loss_G_adv = adv_criterion(adv_out, torch.ones_like(adv_out))\n",
    "                loss_G_cls = cls_criterion(cls_out, target_lbl)\n",
    "                loss_G = loss_G_adv + \\\n",
    "                         lambda_rec * rec_criterion(rec_x, real_x) + \\\n",
    "                         lambda_perc * perc_loss + \\\n",
    "                         lambda_cls * loss_G_cls\n",
    "            scaler.scale(loss_G).backward()\n",
    "            scaler.step(optim_G)\n",
    "            scaler.update()\n",
    "\n",
    "            # collect stats\n",
    "            train_stats['adv'].append(loss_G_adv.item())\n",
    "            train_stats['rec'].append(rec_criterion(rec_x, real_x).item())\n",
    "            train_stats['perc'].append(perc_loss.item())\n",
    "            train_stats['cls'].append(loss_G_cls.item())\n",
    "            pbar.set_postfix({k: np.mean(v) for k, v in train_stats.items()})\n",
    "\n",
    "        # Validation\n",
    "        G.eval(); D.eval()\n",
    "        val_stats = {'adv': [], 'rec': [], 'perc': [], 'cls': []}\n",
    "        with torch.no_grad():\n",
    "            vbar = tqdm(val_loader, desc=f\"Epoch {epoch}/{epochs} [Val]\")\n",
    "            for real_x, real_lbl in vbar:\n",
    "                real_x = real_x.to(device)\n",
    "                B = real_x.size(0)\n",
    "                idx = torch.randperm(B)\n",
    "                target_lbl = real_lbl[idx].to(device)\n",
    "                target_oh = F.one_hot(target_lbl, G.module.n_classes if hasattr(G,'module') else G.n_classes).float().to(device)\n",
    "                fake_x = G(real_x, target_oh)\n",
    "                adv_out, cls_out = D(fake_x)\n",
    "                rec_lbl_oh = F.one_hot(real_lbl.to(device), G.module.n_classes if hasattr(G,'module') else G.n_classes).float().to(device)\n",
    "                rec_x = G(fake_x, rec_lbl_oh)\n",
    "                val_perc = perceptual_loss(fake_x, real_x)\n",
    "                loss_val_cls = cls_criterion(cls_out, target_lbl)\n",
    "                val_stats['adv'].append(adv_criterion(adv_out, torch.ones_like(adv_out)).item())\n",
    "                val_stats['rec'].append(rec_criterion(rec_x, real_x).item())\n",
    "                val_stats['perc'].append(val_perc.item())\n",
    "                val_stats['cls'].append(loss_val_cls.item())\n",
    "                vbar.set_postfix({k: np.mean(v) for k, v in val_stats.items()})\n",
    "\n",
    "        # wandb logging\n",
    "        wandb.log({\n",
    "            'epoch': epoch,\n",
    "            'train/adv': np.mean(train_stats['adv']),\n",
    "            'train/rec': np.mean(train_stats['rec']),\n",
    "            'train/perc': np.mean(train_stats['perc']),\n",
    "            'train/cls': np.mean(train_stats['cls']),\n",
    "            'val/adv': np.mean(val_stats['adv']),\n",
    "            'val/rec': np.mean(val_stats['rec']),\n",
    "            'val/perc': np.mean(val_stats['perc']),\n",
    "            'val/cls': np.mean(val_stats['cls'])\n",
    "        })\n",
    "\n",
    "        if epoch % 3 == 0:\n",
    "            G.eval()\n",
    "            os.makedirs('samples', exist_ok=True)\n",
    "            # sample 7 random images\n",
    "            # handle Subset vs full dataset``\n",
    "            sample_list = random.sample(dataset.files, 7)\n",
    "            num_labels = G.module.n_classes if hasattr(G, 'module') else G.n_classes\n",
    "            fig, axes = plt.subplots(len(sample_list), num_labels+1, figsize=(3*(num_labels+1), 3*len(sample_list)))\n",
    "            for i, (img_path, _) in enumerate(sample_list):\n",
    "                img_orig = Image.open(img_path).convert('RGB')\n",
    "                img_resized = img_orig.resize((256,256))\n",
    "                axes[i, 0].imshow(img_resized); axes[i, 0].axis('off')\n",
    "                for j in range(num_labels):\n",
    "                    x = transform_in(img_orig).unsqueeze(0).to(device)\n",
    "                    lbl = torch.tensor([j], device=device)\n",
    "                    lbl_onehot = F.one_hot(lbl, num_classes=num_labels).float().to(device)\n",
    "                    with torch.no_grad():\n",
    "                        fake = G(x, lbl_onehot)\n",
    "                    fake = unnormalize(fake.squeeze(0).cpu()).clamp(0,1)\n",
    "                    axes[i, j+1].imshow(transforms.ToPILImage()(fake)); axes[i, j+1].axis('off')\n",
    "            os.makedirs(f'samples/{VERSION_NAME}', exist_ok=True)\n",
    "            fig.savefig(os.path.join(f'samples/{VERSION_NAME}', f'epoch_{epoch}.png'))\n",
    "            plt.close(fig)\n",
    "            G.train()\n",
    "\n",
    "        # Save checkpoint\n",
    "        if epoch % 5 == 0 or epoch == epochs:\n",
    "            ckpt_path = os.path.join(checkpoint_dir, f\"ckpt_epoch_{epoch}.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'G_state': G.state_dict(),\n",
    "                'D_state': D.state_dict(),\n",
    "                'optim_G_state': optim_G.state_dict(),\n",
    "                'optim_D_state': optim_D.state_dict()\n",
    "            }, ckpt_path)\n",
    "        metrics['train_adv'].append(np.mean(train_stats['adv']))\n",
    "        metrics['train_rec'].append(np.mean(train_stats['rec']))\n",
    "        metrics['train_perc'].append(np.mean(train_stats['perc']))\n",
    "        metrics['train_cls'].append(np.mean(train_stats['cls']))\n",
    "        metrics['val_adv'].append(np.mean(val_stats['adv']))\n",
    "        metrics['val_rec'].append(np.mean(val_stats['rec']))\n",
    "        metrics['val_perc'].append(np.mean(val_stats['perc']))\n",
    "        metrics['val_cls'].append(np.mean(val_stats['cls']))\n",
    "    # end epochs\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17e86ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhrithik-v\u001b[0m (\u001b[33mhrithik-v-dtu\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250512_174609-6k7i7mtr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hrithik-v-dtu/WeatherGAN/runs/6k7i7mtr' target=\"_blank\">perceptual_included</a></strong> to <a href='https://wandb.ai/hrithik-v-dtu/WeatherGAN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hrithik-v-dtu/WeatherGAN' target=\"_blank\">https://wandb.ai/hrithik-v-dtu/WeatherGAN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hrithik-v-dtu/WeatherGAN/runs/6k7i7mtr' target=\"_blank\">https://wandb.ai/hrithik-v-dtu/WeatherGAN/runs/6k7i7mtr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/80 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [05:02<00:00,  3.12s/it, adv=0.381, rec=0.252, perc=1.11, cls=1.75]\n",
      "Epoch 1/80 [Val]: 0it [00:00, ?it/s]\n",
      "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Epoch 2/80 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [05:02<00:00,  3.12s/it, adv=0.362, rec=0.218, perc=1, cls=1.27]    \n",
      "Epoch 2/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 3/80 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [05:01<00:00,  3.11s/it, adv=0.348, rec=0.206, perc=0.962, cls=0.637]\n",
      "Epoch 3/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 4/80 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [05:02<00:00,  3.12s/it, adv=0.355, rec=0.198, perc=0.925, cls=0.47] \n",
      "Epoch 4/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 5/80 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [05:02<00:00,  3.11s/it, adv=0.361, rec=0.189, perc=0.898, cls=0.411]\n",
      "Epoch 5/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 6/80 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [05:02<00:00,  3.12s/it, adv=0.368, rec=0.18, perc=0.875, cls=0.315] \n",
      "Epoch 6/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 7/80 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [05:01<00:00,  3.11s/it, adv=0.382, rec=0.18, perc=0.866, cls=0.296] \n",
      "Epoch 7/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 8/80 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [05:02<00:00,  3.11s/it, adv=0.377, rec=0.175, perc=0.842, cls=0.263]\n",
      "Epoch 8/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 9/80 [Train]:  33%|‚ñà‚ñà‚ñà‚ñé      | 32/97 [01:45<03:33,  3.28s/it, adv=0.388, rec=0.164, perc=0.813, cls=0.273]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_326/2750432465.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Call train_starGAN function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m metrics = train(\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperceptual_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_326/3158887028.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(G, D, perceptual_loss, train_loader, val_loader, optim_G, optim_D, device, epochs, lambda_rec, lambda_perc, lambda_cls, checkpoint_dir, resume_path, dataset)\u001b[0m\n\u001b[1;32m     86\u001b[0m                          \u001b[0mlambda_cls\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_G_cls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_G\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim_G\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mperceptual_included\u001b[0m at: \u001b[34mhttps://wandb.ai/hrithik-v-dtu/WeatherGAN/runs/6k7i7mtr\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250512_174609-6k7i7mtr/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import wandb\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instantiate models\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "perceptual_loss = VGGPerceptualLoss(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    G = nn.DataParallel(G)\n",
    "    D = nn.DataParallel(D)\n",
    "\n",
    "# Optimizers\n",
    "optim_G = torch.optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "optim_D = torch.optim.Adam(D.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "# Create dataset and split into train and validation sets\n",
    "BATCH_SIZE = 48\n",
    "DATASET_PATH = '/kaggle/working/Weather-Detection-Using-Images/Data'\n",
    "# DATASET_PATH = '/kaggle/input/five-weather-23k'\n",
    "VERSION_NAME = \"perceptual_included\"\n",
    "dataset = ImageFilenameDataset(DATASET_PATH, transform=transform)\n",
    "train_size = int(1.0 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# lambdas\n",
    "lambda_cls = 1.0\n",
    "lambda_rec = 10.0\n",
    "lambda_perc = 1.0\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project='WeatherGAN', name=VERSION_NAME,\n",
    "    resume= \"allow\",\n",
    "    id='6k7i7mtr',\n",
    "    config={\n",
    "    'epochs': 80,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'lr': 2e-4,\n",
    "    'lambda_cls': lambda_cls,\n",
    "    'lambda_rec': lambda_rec,\n",
    "    'lambda_perc': lambda_perc,\n",
    "})\n",
    "\n",
    "# Call train_starGAN function\n",
    "metrics = train(\n",
    "    G, D, perceptual_loss,\n",
    "    train_loader, val_loader,\n",
    "    optim_G, optim_D,\n",
    "    device,\n",
    "    epochs=80,\n",
    "    lambda_cls=lambda_cls,\n",
    "    lambda_rec=lambda_rec,  \n",
    "    lambda_perc=lambda_perc,\n",
    "    checkpoint_dir=f'checkpoints/{VERSION_NAME}',\n",
    "    dataset=dataset,\n",
    "    # resume_path=f\"checkpoints/{VERSION_NAME}/ckpt_epoch_5.pt\"\n",
    ")\n",
    "\n",
    "# No need to save metrics as .pkl, all metrics are logged to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88093a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./checkpoints/perceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d96a88e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b4285",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1981f358",
   "metadata": {},
   "source": [
    "## Inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf2a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference for 10 images from class '0' and all target labels\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 1) define transforms (must match training)\n",
    "transform_in = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "# 2) helper to undo Normalize\n",
    "unnormalize = transforms.Normalize(\n",
    "    mean=[-1.0, -1.0, -1.0],\n",
    "    std =[2.0,  2.0,  2.0]\n",
    ")\n",
    "\n",
    "# 3) load checkpoint & build model\n",
    "ckpt = torch.load('./checkpoints/patchgan_based/ckpt_epoch_5.pt', map_location=device)\n",
    "# If model was trained with DataParallel, wrap before loading state dict\n",
    "\n",
    "G = Generator().to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    G = nn.DataParallel(G)\n",
    "G.load_state_dict(ckpt['G_state'])\n",
    "G.eval()\n",
    "\n",
    "# 4) inference function\n",
    "def infer(img_path, target_label):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    x = transform_in(img).unsqueeze(0).to(device)\n",
    "    lbl = torch.tensor([target_label], device=device)\n",
    "    # handle DataParallel vs single‚ÄêGPU\n",
    "    num_classes = G.module.n_classes if hasattr(G, 'module') else G.n_classes\n",
    "    lbl_onehot = F.one_hot(lbl, num_classes=num_classes).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        fake = G(x, lbl_onehot)\n",
    "    # undo normalization & clamp\n",
    "    fake = unnormalize(fake.squeeze(0).cpu()).clamp(0,1)\n",
    "    return img, transforms.ToPILImage()(fake)\n",
    "\n",
    "# Inference for 10 images from class '0' and all target labels\n",
    "base_path = DATASET_PATH\n",
    "dir0 = os.path.join(base_path, '0')\n",
    "files0 = sorted([f for f in os.listdir(dir0) if f.lower().endswith(('.jpg', '.png'))])\n",
    "files0 = random.sample(files0, min(7, len(files0)))\n",
    "num_labels = G.module.n_classes if hasattr(G, 'module') else G.n_classes\n",
    "fig, axes = plt.subplots(len(files0), 1 + num_labels, figsize=(3*(1+num_labels), 3*len(files0)))\n",
    "for i, fname in enumerate(files0):\n",
    "    path = os.path.join(dir0, fname)\n",
    "    img_orig = Image.open(path).convert('RGB')\n",
    "    img_resized = img_orig.resize((256,256))\n",
    "    axes[i, 0].imshow(img_resized); axes[i, 0].axis('off'); axes[i, 0].set_title('Orig')\n",
    "    for lb in range(num_labels):\n",
    "        _, gen = infer(path, target_label=lb)\n",
    "        axes[i, lb+1].imshow(gen); axes[i, lb+1].axis('off'); axes[i, lb+1].set_title(f'Lbl {lb}')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac44d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
