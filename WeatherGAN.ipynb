{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89ffd954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import torch, random, numpy as np\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "print(f\"Global seed set to {seed}\")\n",
    "# !pip install wandb pytorch-msssim --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17fde331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'checkpoints/wo_perceptual_sa/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls checkpoints/wo_perceptual_sa/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b1e671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login 8285c1e86ba66976957cd9bdbae9e646b37bba8f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d39acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/ayannareda/Weather-Detection-Using-Images.git\n",
    "# url - /kaggle/working/Weather-Detection-Using-Images/Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9d82e",
   "metadata": {},
   "source": [
    "### Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fce2053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ImageFilenameDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.files = []\n",
    "        for label_name in os.listdir(root):\n",
    "            label_dir = os.path.join(root, label_name)\n",
    "            if os.path.isdir(label_dir) and label_name.isdigit():\n",
    "                for fname in sorted(os.listdir(label_dir))[:2200]:\n",
    "                    if fname.lower().endswith(('.jpg', '.png')):\n",
    "                        path = os.path.join(label_dir, fname)\n",
    "                        self.files.append((path, int(label_name)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.files[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa737a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/kaggle/input/five-weather-23k/3/1373.jpg', 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "dataset = ImageFilenameDataset('/kaggle/input/five-weather-23k', transform=transform)\n",
    "dataset.files[5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1582a4b6",
   "metadata": {},
   "source": [
    "### Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ecbf884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StarGAN Generator with ResNet blocks and Self-Attention\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual Block with instance normalization.\"\"\"\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out, affine=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.main(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator network.\"\"\"\n",
    "    def __init__(self, conv_dim=64, n_classes=5, repeat_num=4):\n",
    "        super(Generator, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(3+self.n_classes, conv_dim, kernel_size=7, stride=1, padding=3, bias=False))\n",
    "        layers.append(nn.InstanceNorm2d(conv_dim, affine=True))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        # Down-sampling layers.\n",
    "        curr_dim = conv_dim\n",
    "        for i in range(2):\n",
    "            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1, bias=False))\n",
    "            layers.append(nn.InstanceNorm2d(curr_dim*2, affine=True))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            curr_dim = curr_dim * 2\n",
    "\n",
    "        # Bottleneck layers.\n",
    "        for i in range(repeat_num):\n",
    "            layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))\n",
    "\n",
    "        # Up-sampling layers.\n",
    "        for i in range(2):\n",
    "            layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "            layers.append(nn.Conv2d(curr_dim, curr_dim//2, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "            layers.append(nn.InstanceNorm2d(curr_dim//2, affine=True))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            curr_dim = curr_dim // 2\n",
    "\n",
    "        layers.append(nn.Conv2d(curr_dim, 3, kernel_size=7, stride=1, padding=3, bias=False))\n",
    "        layers.append(nn.Tanh())\n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # Replicate spatially and concatenate domain information.\n",
    "        # Note that this type of label conditioning does not work at all if we use reflection padding in Conv2d.\n",
    "        # This is because instance normalization ignores the shifting (or bias) effect.\n",
    "        c = c.view(c.size(0), c.size(1), 1, 1)\n",
    "        c = c.repeat(1, 1, x.size(2), x.size(3))\n",
    "        x = torch.cat([x, c], dim=1)\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e205b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PatchGAN Discriminator\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import spectral_norm\n",
    "import numpy as np\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator network with PatchGAN.\"\"\"\n",
    "    def __init__(self, image_size=256, conv_dim=64, c_dim=5, repeat_num=4):\n",
    "        super(Discriminator, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(spectral_norm(nn.Conv2d(3, conv_dim, kernel_size=4, stride=2, padding=1)))\n",
    "        layers.append(nn.LeakyReLU(0.01))\n",
    "\n",
    "        curr_dim = conv_dim\n",
    "        for i in range(1, repeat_num):\n",
    "            layers.append(spectral_norm(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1)))\n",
    "            layers.append(nn.LeakyReLU(0.01))\n",
    "            curr_dim = curr_dim * 2\n",
    "\n",
    "        kernel_size = int(image_size / np.power(2, repeat_num))\n",
    "        self.main = nn.Sequential(*layers)\n",
    "        self.conv1 = spectral_norm(nn.Conv2d(curr_dim, 1, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "        self.conv2 = spectral_norm(nn.Conv2d(curr_dim, c_dim, kernel_size=kernel_size, bias=False))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.main(x)\n",
    "        out_src = self.conv1(h)\n",
    "        out_cls = self.conv2(h)\n",
    "        return out_src, out_cls.view(out_cls.size(0), out_cls.size(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc0fe2a",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85c5918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from pytorch_msssim import ssim\n",
    "import wandb\n",
    "from torch.amp import autocast, GradScaler\n",
    "import os, random\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define inference transforms and unnormalize\n",
    "transform_in = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "unnormalize = transforms.Normalize(\n",
    "    mean=[-1.0, -1.0, -1.0],\n",
    "    std =[2.0,  2.0,  2.0]\n",
    ")\n",
    "\n",
    "def train(G, D, train_loader, val_loader, optim_G, optim_D, device,\n",
    "          epochs=50, lambda_rec=10.0, lambda_perc=0.1, lambda_ssim=1.0, lambda_cls=1.0,\n",
    "          checkpoint_dir='checkpoints', resume_path=None, dataset=None):\n",
    "    # initialize metrics storage\n",
    "    metrics = {\n",
    "        'train_adv': [], 'train_rec': [], 'train_perc': [], 'train_ssim': [], 'train_cls': [],\n",
    "        'val_adv':   [], 'val_rec':   [], 'val_perc':   [], 'val_ssim':   [], 'val_cls': []\n",
    "    }\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    start_epoch = 1\n",
    "    # -- resume if provided --\n",
    "    if resume_path and os.path.exists(resume_path):\n",
    "        ckpt = torch.load(resume_path, map_location=device)\n",
    "        G.load_state_dict(ckpt['G_state']); D.load_state_dict(ckpt['D_state'])\n",
    "        optim_G.load_state_dict(ckpt['optim_G_state']); optim_D.load_state_dict(ckpt['optim_D_state'])\n",
    "        start_epoch = ckpt.get('epoch', 1) + 1\n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "    adv_criterion = torch.nn.MSELoss()\n",
    "    rec_criterion = torch.nn.L1Loss()\n",
    "    cls_criterion = torch.nn.CrossEntropyLoss()\n",
    "    G.to(device); D.to(device)\n",
    "    scaler = GradScaler('cuda')\n",
    "\n",
    "    for epoch in range(start_epoch, epochs+1):\n",
    "        G.train(); D.train()\n",
    "        train_stats = {'adv': [], 'rec': [], 'perc': [], 'ssim': [], 'cls': []}\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} [Train]\")\n",
    "        for real_x, real_lbl in pbar:\n",
    "            real_x = real_x.to(device)\n",
    "            B = real_x.size(0)\n",
    "            idx = torch.randperm(B)\n",
    "            target_lbl = real_lbl[idx].to(device)\n",
    "            target_oh = F.one_hot(target_lbl, G.module.n_classes if hasattr(G,'module') else G.n_classes).float().to(device)\n",
    "\n",
    "            # Discriminator update\n",
    "            optim_D.zero_grad()\n",
    "            with autocast('cuda'):\n",
    "                fake_x = G(real_x, target_oh).detach()\n",
    "                real_src, real_cls = D(real_x)\n",
    "                fake_src, _ = D(fake_x)\n",
    "                loss_D_adv = adv_criterion(real_src, torch.ones_like(real_src)) + \\\n",
    "                             adv_criterion(fake_src, torch.zeros_like(fake_src))\n",
    "                loss_D_cls = cls_criterion(real_cls, real_lbl.to(device))\n",
    "                loss_D = loss_D_adv + lambda_cls * loss_D_cls\n",
    "            scaler.scale(loss_D).backward()\n",
    "            scaler.step(optim_D)\n",
    "            scaler.update()\n",
    "\n",
    "            # Generator update\n",
    "            optim_G.zero_grad()\n",
    "            with autocast('cuda'):\n",
    "                fake_x = G(real_x, target_oh)\n",
    "                adv_out, cls_out = D(fake_x)\n",
    "                rec_lbl_oh = F.one_hot(real_lbl.to(device), G.module.n_classes if hasattr(G,'module') else G.n_classes).float().to(device)\n",
    "                rec_x = G(fake_x, rec_lbl_oh)\n",
    "                perc_loss = 0\n",
    "                ssim_loss = 1 - ssim(fake_x, real_x, data_range=2.0, size_average=True)\n",
    "                loss_G_adv = adv_criterion(adv_out, torch.ones_like(adv_out))\n",
    "                loss_G_cls = cls_criterion(cls_out, target_lbl)\n",
    "                loss_G = loss_G_adv + \\\n",
    "                         lambda_rec * rec_criterion(rec_x, real_x) + \\\n",
    "                         lambda_perc * perc_loss + \\\n",
    "                         lambda_ssim * ssim_loss + \\\n",
    "                         lambda_cls * loss_G_cls\n",
    "            scaler.scale(loss_G).backward()\n",
    "            scaler.step(optim_G)\n",
    "            scaler.update()\n",
    "\n",
    "            # collect stats\n",
    "            train_stats['adv'].append(loss_G_adv.item())\n",
    "            train_stats['rec'].append(rec_criterion(rec_x, real_x).item())\n",
    "            train_stats['perc'].append(perc_loss)\n",
    "            train_stats['ssim'].append((1 - ssim_loss).item())\n",
    "            train_stats['cls'].append(loss_G_cls.item())\n",
    "            pbar.set_postfix({k: np.mean(v) for k, v in train_stats.items()})\n",
    "\n",
    "        # Validation\n",
    "        G.eval(); D.eval()\n",
    "        val_stats = {'adv': [], 'rec': [], 'perc': [], 'ssim': [], 'cls': []}\n",
    "        with torch.no_grad():\n",
    "            vbar = tqdm(val_loader, desc=f\"Epoch {epoch}/{epochs} [Val]\")\n",
    "            for real_x, real_lbl in vbar:\n",
    "                real_x = real_x.to(device)\n",
    "                B = real_x.size(0)\n",
    "                idx = torch.randperm(B)\n",
    "                target_lbl = real_lbl[idx].to(device)\n",
    "                target_oh = F.one_hot(target_lbl, G.module.n_classes if hasattr(G,'module') else G.n_classes).float().to(device)\n",
    "                fake_x = G(real_x, target_oh)\n",
    "                adv_out, cls_out = D(fake_x)\n",
    "                rec_lbl_oh = F.one_hot(real_lbl.to(device), G.module.n_classes if hasattr(G,'module') else G.n_classes).float().to(device)\n",
    "                rec_x = G(fake_x, rec_lbl_oh)\n",
    "                val_perc = 0\n",
    "                val_ssim = ssim(fake_x, real_x, data_range=2.0, size_average=True)\n",
    "                loss_val_cls = cls_criterion(cls_out, target_lbl)\n",
    "                val_stats['adv'].append(adv_criterion(adv_out, torch.ones_like(adv_out)).item())\n",
    "                val_stats['rec'].append(rec_criterion(rec_x, real_x).item())\n",
    "                val_stats['perc'].append(val_perc)\n",
    "                val_stats['ssim'].append(val_ssim.item())\n",
    "                val_stats['cls'].append(loss_val_cls.item())\n",
    "                vbar.set_postfix({k: np.mean(v) for k, v in val_stats.items()})\n",
    "\n",
    "        # wandb logging\n",
    "        wandb.log({\n",
    "            'epoch': epoch,\n",
    "            'train/adv': np.mean(train_stats['adv']),\n",
    "            'train/rec': np.mean(train_stats['rec']),\n",
    "            'train/perc': np.mean(train_stats['perc']),\n",
    "            'train/ssim': np.mean(train_stats['ssim']),\n",
    "            'train/cls': np.mean(train_stats['cls']),\n",
    "            'val/adv': np.mean(val_stats['adv']),\n",
    "            'val/rec': np.mean(val_stats['rec']),\n",
    "            'val/perc': np.mean(val_stats['perc']),\n",
    "            'val/ssim': np.mean(val_stats['ssim']),\n",
    "            'val/cls': np.mean(val_stats['cls'])\n",
    "        })\n",
    "\n",
    "        if epoch % 3 == 0:\n",
    "            G.eval()\n",
    "            os.makedirs('samples', exist_ok=True)\n",
    "            # sample 7 random images\n",
    "            # handle Subset vs full dataset``\n",
    "            sample_list = random.sample(dataset.files, 7)\n",
    "            num_labels = G.module.n_classes if hasattr(G, 'module') else G.n_classes\n",
    "            fig, axes = plt.subplots(len(sample_list), num_labels+1, figsize=(3*(num_labels+1), 3*len(sample_list)))\n",
    "            for i, (img_path, _) in enumerate(sample_list):\n",
    "                img_orig = Image.open(img_path).convert('RGB')\n",
    "                img_resized = img_orig.resize((256,256))\n",
    "                axes[i, 0].imshow(img_resized); axes[i, 0].axis('off')\n",
    "                for j in range(num_labels):\n",
    "                    x = transform_in(img_orig).unsqueeze(0).to(device)\n",
    "                    lbl = torch.tensor([j], device=device)\n",
    "                    lbl_onehot = F.one_hot(lbl, num_classes=num_labels).float().to(device)\n",
    "                    with torch.no_grad():\n",
    "                        fake = G(x, lbl_onehot)\n",
    "                    fake = unnormalize(fake.squeeze(0).cpu()).clamp(0,1)\n",
    "                    axes[i, j+1].imshow(transforms.ToPILImage()(fake)); axes[i, j+1].axis('off')\n",
    "            fig.savefig(os.path.join('samples', f'epoch_{epoch}.png'))\n",
    "            plt.close(fig)\n",
    "            G.train()\n",
    "\n",
    "        # Save checkpoint\n",
    "        if epoch % 5 == 0 or epoch == epochs:\n",
    "            ckpt_path = os.path.join(checkpoint_dir, f\"ckpt_epoch_{epoch}.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'G_state': G.state_dict(),\n",
    "                'D_state': D.state_dict(),\n",
    "                'optim_G_state': optim_G.state_dict(),\n",
    "                'optim_D_state': optim_D.state_dict()\n",
    "            }, ckpt_path)\n",
    "        metrics['train_adv'].append(np.mean(train_stats['adv']))\n",
    "        metrics['train_rec'].append(np.mean(train_stats['rec']))\n",
    "        metrics['train_perc'].append(np.mean(train_stats['perc']))\n",
    "        metrics['train_ssim'].append(np.mean(train_stats['ssim']))\n",
    "        metrics['train_cls'].append(np.mean(train_stats['cls']))\n",
    "        metrics['val_adv'].append(np.mean(val_stats['adv']))\n",
    "        metrics['val_rec'].append(np.mean(val_stats['rec']))\n",
    "        metrics['val_perc'].append(np.mean(val_stats['perc']))\n",
    "        metrics['val_ssim'].append(np.mean(val_stats['ssim']))\n",
    "        metrics['val_cls'].append(np.mean(val_stats['cls']))\n",
    "    # end epochs\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1f45da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb pytorch-msssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e86ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhrithik-v\u001b[0m (\u001b[33mhrithik-v-dtu\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250512_122454-ly3f63qx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hrithik-v-dtu/WeatherGAN/runs/ly3f63qx' target=\"_blank\">stargan-patchgan</a></strong> to <a href='https://wandb.ai/hrithik-v-dtu/WeatherGAN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hrithik-v-dtu/WeatherGAN' target=\"_blank\">https://wandb.ai/hrithik-v-dtu/WeatherGAN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hrithik-v-dtu/WeatherGAN/runs/ly3f63qx' target=\"_blank\">https://wandb.ai/hrithik-v-dtu/WeatherGAN/runs/ly3f63qx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/80 [Train]: 100%|██████████| 56/56 [02:13<00:00,  2.39s/it, adv=0.363, rec=0.264, perc=0, ssim=0.45, cls=1.75]  \n",
      "Epoch 1/80 [Val]: 0it [00:00, ?it/s]\n",
      "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Epoch 2/80 [Train]: 100%|██████████| 56/56 [02:16<00:00,  2.44s/it, adv=0.407, rec=0.228, perc=0, ssim=0.523, cls=1.56]\n",
      "Epoch 2/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 3/80 [Train]: 100%|██████████| 56/56 [02:17<00:00,  2.45s/it, adv=0.369, rec=0.229, perc=0, ssim=0.53, cls=1.01] \n",
      "Epoch 3/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 4/80 [Train]: 100%|██████████| 56/56 [02:16<00:00,  2.44s/it, adv=0.384, rec=0.214, perc=0, ssim=0.53, cls=0.703] \n",
      "Epoch 4/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 5/80 [Train]: 100%|██████████| 56/56 [02:16<00:00,  2.44s/it, adv=0.404, rec=0.209, perc=0, ssim=0.538, cls=0.466]\n",
      "Epoch 5/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 6/80 [Train]: 100%|██████████| 56/56 [02:16<00:00,  2.44s/it, adv=0.395, rec=0.204, perc=0, ssim=0.545, cls=0.548]\n",
      "Epoch 6/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 7/80 [Train]: 100%|██████████| 56/56 [02:16<00:00,  2.44s/it, adv=0.355, rec=0.189, perc=0, ssim=0.561, cls=0.614]\n",
      "Epoch 7/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 8/80 [Train]: 100%|██████████| 56/56 [02:16<00:00,  2.44s/it, adv=0.402, rec=0.196, perc=0, ssim=0.55, cls=0.361] \n",
      "Epoch 8/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 9/80 [Train]: 100%|██████████| 56/56 [02:16<00:00,  2.43s/it, adv=0.421, rec=0.208, perc=0, ssim=0.558, cls=0.477]\n",
      "Epoch 9/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 10/80 [Train]: 100%|██████████| 56/56 [02:17<00:00,  2.45s/it, adv=0.398, rec=0.185, perc=0, ssim=0.578, cls=0.523]\n",
      "Epoch 10/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 11/80 [Train]: 100%|██████████| 56/56 [02:16<00:00,  2.44s/it, adv=0.401, rec=0.181, perc=0, ssim=0.575, cls=0.321]\n",
      "Epoch 11/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 12/80 [Train]: 100%|██████████| 56/56 [02:16<00:00,  2.43s/it, adv=0.384, rec=0.183, perc=0, ssim=0.56, cls=0.268] \n",
      "Epoch 12/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 13/80 [Train]: 100%|██████████| 56/56 [02:17<00:00,  2.46s/it, adv=0.385, rec=0.187, perc=0, ssim=0.564, cls=0.352]\n",
      "Epoch 13/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 14/80 [Train]: 100%|██████████| 56/56 [02:17<00:00,  2.45s/it, adv=0.367, rec=0.172, perc=0, ssim=0.579, cls=0.309]\n",
      "Epoch 14/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 15/80 [Train]: 100%|██████████| 56/56 [02:15<00:00,  2.43s/it, adv=0.372, rec=0.17, perc=0, ssim=0.581, cls=0.224] \n",
      "Epoch 15/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 16/80 [Train]: 100%|██████████| 56/56 [02:16<00:00,  2.43s/it, adv=0.383, rec=0.161, perc=0, ssim=0.596, cls=0.197]\n",
      "Epoch 16/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 18/80 [Train]: 100%|██████████| 56/56 [02:16<00:00,  2.43s/it, adv=0.385, rec=0.158, perc=0, ssim=0.587, cls=0.227]\n",
      "Epoch 18/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 19/80 [Train]: 100%|██████████| 56/56 [02:16<00:00,  2.44s/it, adv=0.39, rec=0.173, perc=0, ssim=0.58, cls=0.183]  \n",
      "Epoch 19/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 20/80 [Train]: 100%|██████████| 56/56 [02:16<00:00,  2.43s/it, adv=0.394, rec=0.161, perc=0, ssim=0.583, cls=0.17] \n",
      "Epoch 20/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 21/80 [Train]: 100%|██████████| 56/56 [02:16<00:00,  2.43s/it, adv=0.382, rec=0.161, perc=0, ssim=0.584, cls=0.275]\n",
      "Epoch 21/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 22/80 [Train]: 100%|██████████| 56/56 [02:16<00:00,  2.43s/it, adv=0.394, rec=0.144, perc=0, ssim=0.611, cls=0.185]\n",
      "Epoch 22/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 23/80 [Train]: 100%|██████████| 56/56 [02:16<00:00,  2.43s/it, adv=0.384, rec=0.153, perc=0, ssim=0.608, cls=0.242]\n",
      "Epoch 23/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 24/80 [Train]: 100%|██████████| 56/56 [02:15<00:00,  2.42s/it, adv=0.386, rec=0.142, perc=0, ssim=0.626, cls=0.164]\n",
      "Epoch 24/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 25/80 [Train]: 100%|██████████| 56/56 [02:15<00:00,  2.43s/it, adv=0.396, rec=0.164, perc=0, ssim=0.603, cls=0.176]\n",
      "Epoch 25/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 26/80 [Train]: 100%|██████████| 56/56 [02:15<00:00,  2.42s/it, adv=0.411, rec=0.188, perc=0, ssim=0.577, cls=0.236]\n",
      "Epoch 26/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 27/80 [Train]: 100%|██████████| 56/56 [02:16<00:00,  2.44s/it, adv=0.384, rec=0.147, perc=0, ssim=0.616, cls=0.171]\n",
      "Epoch 27/80 [Val]: 0it [00:00, ?it/s]\n",
      "Epoch 28/80 [Train]:  88%|████████▊ | 49/56 [02:01<00:17,  2.44s/it, adv=0.378, rec=0.142, perc=0, ssim=0.618, cls=0.181]"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "# Example usage of train_starGAN function\n",
    "import wandb\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instantiate models\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    G = nn.DataParallel(G)\n",
    "    D = nn.DataParallel(D)\n",
    "\n",
    "# Optimizers\n",
    "optim_G = torch.optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "optim_D = torch.optim.Adam(D.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "# Create dataset and split into train and validation sets\n",
    "BATCH_SIZE = 84\n",
    "DATASET_PATH = '/kaggle/working/Weather-Detection-Using-Images/Data'\n",
    "# DATASET_PATH = '/kaggle/input/five-weather-23k'\n",
    "dataset = ImageFilenameDataset(DATASET_PATH, transform=transform)\n",
    "train_size = int(1.0 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# lambdas\n",
    "lambda_cls = 1.0\n",
    "lambda_rec = 10.0\n",
    "lambda_ssim = 1.0\n",
    "lambda_perc = 0\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project='WeatherGAN', name='stargan-patchgan',\n",
    "    # resume= \"allow\",\n",
    "    # id='sscjv01f',\n",
    "    config={\n",
    "    'epochs': 80,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'lr': 2e-4,\n",
    "    'lambda_cls': lambda_cls,\n",
    "    'lambda_rec': lambda_rec,\n",
    "    'lambda_perc': lambda_perc,\n",
    "    'lambda_ssim': lambda_ssim,\n",
    "})\n",
    "\n",
    "# Call train_starGAN function\n",
    "metrics = train(\n",
    "    G, D,\n",
    "    train_loader, val_loader,\n",
    "    optim_G, optim_D,\n",
    "    device,\n",
    "    epochs=80,\n",
    "    lambda_cls=lambda_cls,\n",
    "    lambda_rec=lambda_rec,  \n",
    "    lambda_perc=lambda_perc,\n",
    "    lambda_ssim=lambda_ssim,\n",
    "    checkpoint_dir='checkpoints/patchgan_based',\n",
    "    dataset=dataset,\n",
    "    # resume_path=\"checkpoints/patchgan_based/ckpt_epoch_5.pt\"\n",
    ")\n",
    "\n",
    "# No need to save metrics as .pkl, all metrics are logged to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88093a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./checkpoints/perceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d96a88e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b4285",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1981f358",
   "metadata": {},
   "source": [
    "## Inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf2a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference for 10 images from class '0' and all target labels\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 1) define transforms (must match training)\n",
    "transform_in = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "# 2) helper to undo Normalize\n",
    "unnormalize = transforms.Normalize(\n",
    "    mean=[-1.0, -1.0, -1.0],\n",
    "    std =[2.0,  2.0,  2.0]\n",
    ")\n",
    "\n",
    "# 3) load checkpoint & build model\n",
    "ckpt = torch.load('./checkpoints/patchgan_based/ckpt_epoch_5.pt', map_location=device)\n",
    "# If model was trained with DataParallel, wrap before loading state dict\n",
    "\n",
    "G = Generator().to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    G = nn.DataParallel(G)\n",
    "G.load_state_dict(ckpt['G_state'])\n",
    "G.eval()\n",
    "\n",
    "# 4) inference function\n",
    "def infer(img_path, target_label):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    x = transform_in(img).unsqueeze(0).to(device)\n",
    "    lbl = torch.tensor([target_label], device=device)\n",
    "    # handle DataParallel vs single‐GPU\n",
    "    num_classes = G.module.n_classes if hasattr(G, 'module') else G.n_classes\n",
    "    lbl_onehot = F.one_hot(lbl, num_classes=num_classes).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        fake = G(x, lbl_onehot)\n",
    "    # undo normalization & clamp\n",
    "    fake = unnormalize(fake.squeeze(0).cpu()).clamp(0,1)\n",
    "    return img, transforms.ToPILImage()(fake)\n",
    "\n",
    "# Inference for 10 images from class '0' and all target labels\n",
    "base_path = DATASET_PATH\n",
    "dir0 = os.path.join(base_path, '0')\n",
    "files0 = sorted([f for f in os.listdir(dir0) if f.lower().endswith(('.jpg', '.png'))])\n",
    "files0 = random.sample(files0, min(7, len(files0)))\n",
    "num_labels = G.module.n_classes if hasattr(G, 'module') else G.n_classes\n",
    "fig, axes = plt.subplots(len(files0), 1 + num_labels, figsize=(3*(1+num_labels), 3*len(files0)))\n",
    "for i, fname in enumerate(files0):\n",
    "    path = os.path.join(dir0, fname)\n",
    "    img_orig = Image.open(path).convert('RGB')\n",
    "    img_resized = img_orig.resize((256,256))\n",
    "    axes[i, 0].imshow(img_resized); axes[i, 0].axis('off'); axes[i, 0].set_title('Orig')\n",
    "    for lb in range(num_labels):\n",
    "        _, gen = infer(path, target_label=lb)\n",
    "        axes[i, lb+1].imshow(gen); axes[i, lb+1].axis('off'); axes[i, lb+1].set_title(f'Lbl {lb}')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac44d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
